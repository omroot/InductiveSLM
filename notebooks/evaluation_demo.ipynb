{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Evaluation Pipeline Demo\n",
    "\n",
    "This notebook demonstrates how to use the `evaluate_model_pipeline` function to:\n",
    "1. Evaluate a baseline model\n",
    "2. Fine-tune the model with LoRA\n",
    "3. Evaluate the fine-tuned model\n",
    "4. Save all results and adapters to an output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "autoreload",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warnings",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "path",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import set_seed\n",
    "\n",
    "from src.preprocess.deer import DeerToTriplets\n",
    "from src.preprocess.utils import to_text\n",
    "from src.models.evaluate import evaluate_model_pipeline\n",
    "from src.utils.io.read import RawDataReader\n",
    "from src.settings import Settings\n",
    "import src.config as cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set HuggingFace token\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = cfg.HUGGINGFACE_HUB_TOKEN\n",
    "HF_TOKEN = os.environ[\"HUGGINGFACE_HUB_TOKEN\"]\n",
    "\n",
    "# Model and training configuration\n",
    "MODEL_ID = cfg.MODEL_ID\n",
    "OUTPUT_DIR = cfg.OUTPUT_DIR\n",
    "SEED = cfg.SEED\n",
    "VAL_FRACTION = cfg.VAL_FRACTION\n",
    "MAX_SEQ_LEN = cfg.MAX_SEQ_LEN\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_R = cfg.LORA_R\n",
    "LORA_ALPHA = cfg.LORA_ALPHA\n",
    "LORA_DROPOUT = cfg.LORA_DROPOUT\n",
    "TARGET_MODULES = cfg.TARGET_MODULES\n",
    "\n",
    "# Training Configuration\n",
    "LR = cfg.LR\n",
    "EPOCHS = cfg.EPOCHS\n",
    "BATCH_SIZE = cfg.BATCH_SIZE\n",
    "GRAD_ACCUM = cfg.GRAD_ACCUM\n",
    "LOG_STEPS = cfg.LOG_STEPS\n",
    "EVAL_STEPS = cfg.EVAL_STEPS\n",
    "SAVE_STEPS = cfg.SAVE_STEPS\n",
    "GEN_MAX_NEW_TOKENS = cfg.GEN_MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "rdr = RawDataReader(Settings.paths.RAW_DATA_PATH)\n",
    "ir_triplets_dataset = rdr.read_ir_triplets()\n",
    "deer_dataset = rdr.read_deer()\n",
    "\n",
    "# Convert DEER to triplets\n",
    "deer_to_triplets_converter = DeerToTriplets()\n",
    "deer_to_triplets_converter.process(deer_dataset)\n",
    "od_val_data = deer_to_triplets_converter.triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# Split data into train and in-distribution validation\n",
    "data = ir_triplets_dataset\n",
    "random.Random(SEED).shuffle(data)\n",
    "split_idx = int(len(data) * (1 - VAL_FRACTION))\n",
    "train_raw, id_val_raw = data[:split_idx], data[split_idx:]\n",
    "od_val_raw = od_val_data\n",
    "\n",
    "print(f\"Training examples: {len(train_raw)}\")\n",
    "print(f\"In-distribution validation examples: {len(id_val_raw)}\")\n",
    "print(f\"Out-of-distribution validation examples: {len(od_val_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_ds = Dataset.from_list([to_text(x) for x in train_raw])\n",
    "id_val_ds = Dataset.from_list([to_text(x) for x in id_val_raw])\n",
    "id_dataset = DatasetDict({\"train\": train_ds, \"validation\": id_val_ds})\n",
    "\n",
    "od_val_ds = Dataset.from_list([to_text(x) for x in od_val_raw])\n",
    "od_dataset = DatasetDict({\"train\": train_ds, \"validation\": od_val_ds})\n",
    "\n",
    "print(\"Datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## Run Complete Evaluation Pipeline\n",
    "\n",
    "The `evaluate_model_pipeline` function will:\n",
    "1. Load the baseline model and evaluate it\n",
    "2. Fine-tune the model with LoRA\n",
    "3. Evaluate the fine-tuned model\n",
    "4. Save all results, predictions, and adapters to the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model_pipeline(\n",
    "    model_id=MODEL_ID,\n",
    "    id_dataset=id_dataset,\n",
    "    od_dataset=od_dataset,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    lora_r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    grad_accum=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    epochs=EPOCHS,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    log_steps=LOG_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    hf_token=HF_TOKEN,\n",
    "    skip_finetuning=False  # Set to True to only run baseline evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "## Access Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "access_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access individual metrics\n",
    "print(\"\\nIn-Distribution Baseline Metrics:\")\n",
    "print(results[\"id_baseline_metrics\"])\n",
    "\n",
    "print(\"\\nIn-Distribution Fine-tuned Metrics:\")\n",
    "print(results[\"id_finetuned_metrics\"])\n",
    "\n",
    "if results[\"od_baseline_metrics\"]:\n",
    "    print(\"\\nOut-of-Distribution Baseline Metrics:\")\n",
    "    print(results[\"od_baseline_metrics\"])\n",
    "    \n",
    "    print(\"\\nOut-of-Distribution Fine-tuned Metrics:\")\n",
    "    print(results[\"od_finetuned_metrics\"])\n",
    "\n",
    "print(f\"\\nAll results saved to: {results['output_dir']}\")\n",
    "print(f\"Model adapters saved to: {results['adapter_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "files",
   "metadata": {},
   "source": [
    "## Output Files\n",
    "\n",
    "The pipeline creates the following files in the output directory:\n",
    "\n",
    "- `id_val_predictions_baseline.jsonl` - Baseline predictions on in-distribution validation set\n",
    "- `id_val_predictions_finetuned.jsonl` - Fine-tuned predictions on in-distribution validation set\n",
    "- `od_val_predictions_baseline.jsonl` - Baseline predictions on out-of-distribution validation set\n",
    "- `od_val_predictions_finetuned.jsonl` - Fine-tuned predictions on out-of-distribution validation set\n",
    "- `metrics_summary.json` - Complete metrics summary with improvements\n",
    "- `adapter/` - Directory containing the fine-tuned LoRA adapters and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional",
   "metadata": {},
   "source": [
    "## Optional: Baseline Evaluation Only\n",
    "\n",
    "If you only want to evaluate the baseline model without fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_only",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run baseline evaluation only\n",
    "# baseline_results = evaluate_model_pipeline(\n",
    "#     model_id=MODEL_ID,\n",
    "#     id_dataset=id_dataset,\n",
    "#     od_dataset=od_dataset,\n",
    "#     output_dir=\"./baseline_eval_only\",\n",
    "#     hf_token=HF_TOKEN,\n",
    "#     skip_finetuning=True\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
